# DLOps Data Pipeline (Streamlit + MongoDB + Faker)

This project demonstrates an end-to-end **Data Engineering pipeline** with:

- Fake data generation (`Faker`)
- ETL pipeline (`pandas`)
- MongoDB storage
- Streamlit UI to run everything

---

## ğŸ“ Structure

- `generate_data.py`: creates 100K fake records
- `main.py`: runs ETL and loads MongoDB
- `app.py`: Streamlit UI
- `pipeline/`: modular ETL scripts

---

## âš™ï¸ Setup Instructions

1. Clone the repo and `cd` into it
2. Install dependencies:

```bash
pip install -r requirements.txt
```
## Start MongoDB (local or Docker):
```bash
docker run -d -p 27017:27017 --name mongo mongo
```
## Run the Streamlit App:
```bash
streamlit run app.py
```
## Output
* data/raw_data.csv â€” Raw fake dataset
* output/cleaned_data.csv â€” Final CSV
* MongoDB â€” dlopsdb.cleaned_data collection

## Folder Structure
```bash
dlops-data-pipeline/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_data.csv
â”œâ”€â”€ output/
â”‚   â””â”€â”€ cleaned_data.csv
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â””â”€â”€ mongo_loader.py  â† new
â”œâ”€â”€ main.py
â”œâ”€â”€ app.py               â† Streamlit UI
â”œâ”€â”€ requirements.txt
```

##  ğŸ“ˆExtensions (Next Steps)
* Add FastAPI interface
* Add MLflow for tracking
* Add Prefect or Airflow orchestration
* Visualize MongoDB data in Streamlit

## yaml

---

## âœ… Final Output Example

After clicking "ğŸ“¦ Generate & Process Data":

- Youâ€™ll see 100K records processed
- Output:  
  âœ… Cleaned CSV at `output/cleaned_data.csv`  
  âœ… MongoDB loaded: `dlopsdb.cleaned_data`  
  âœ… Streamlit preview of 1,000 records

---

Would you like:
- A GitHub repo version of this?
- Dockerfile to containerize the whole project?
- Add MongoDB â†’ Streamlit viewer?


